name: Deploy to DigitalOcean

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target environment for deployment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      tag:
        description: 'Docker image tag to deploy (default: latest)'
        required: false
        default: 'latest'
        type: string

  push:
    branches:
      - master
      - feature/cicd  # –í—Ä–µ–º–µ–Ω–Ω–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    paths-ignore:
      - '**.md'
      - 'docs/**'

permissions:
  contents: read
  packages: read

jobs:
  prepare:
    name: Prepare for Deployment
    runs-on: ubuntu-latest
    outputs:
      environment: ${{ steps.set-env.outputs.environment }}
      tag: ${{ steps.set-tag.outputs.tag }}
    steps:
      - name: Set environment
        id: set-env
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "environment=${{ github.event.inputs.environment }}" >> $GITHUB_OUTPUT
          else
            # Auto-deploy only to staging environment on push to master
            echo "environment=staging" >> $GITHUB_OUTPUT
          fi

      - name: Set image tag
        id: set-tag
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ] && [ "${{ github.event.inputs.tag }}" != "latest" ]; then
            echo "tag=${{ github.event.inputs.tag }}" >> $GITHUB_OUTPUT
          else
            # For automated pushes to master, use the commit SHA
            echo "tag=sha-${{ github.sha }}" >> $GITHUB_OUTPUT
          fi

  terraform:
    name: Deploy Infrastructure
    runs-on: ubuntu-latest
    needs: prepare
    # Use GitHub Environment - this enables environment-specific secrets and protection rules
    environment: ${{ needs.prepare.outputs.environment }}
    outputs:
      token: ${{ steps.vault-token.outputs.token }}
      droplet_ip: ${{ steps.terraform-outputs.outputs.droplet_ip }}
      app_url: ${{ steps.terraform-outputs.outputs.app_url }}
      ssh_port: ${{ steps.terraform-outputs.outputs.ssh_port }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Vault
        uses: ./.github/actions/setup-vault
        env:
          VAULT_SERVER_IP: ${{ secrets.VAULT_SERVER_IP }}

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.7.5

      - name: Configure AWS CLI for DigitalOcean Spaces
        run: |
          # Install AWS CLI if not present (usually pre-installed in GitHub runners)
          if ! command -v aws &> /dev/null; then
            echo "Installing AWS CLI..."
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip awscliv2.zip
            sudo ./aws/install
          else
            echo "AWS CLI already installed: $(aws --version)"
          fi
          
          # Configure AWS CLI for DigitalOcean Spaces
          aws configure set aws_access_key_id "${{ secrets.AWS_ACCESS_KEY_ID }}"
          aws configure set aws_secret_access_key "${{ secrets.AWS_SECRET_ACCESS_KEY }}"
          aws configure set default.region fra1
          
          echo "‚úÖ AWS CLI configured for DigitalOcean Spaces"

      - name: Get Vault Token
        id: vault-token
        env:
          VAULT_ADDR: ${{ secrets.VAULT_ADDR }}
          VAULT_ROLE_ID: ${{ secrets.VAULT_ROLE_ID }}
          VAULT_SECRET_ID: ${{ secrets.VAULT_SECRET_ID }}
        run: |
          # Create temporary files for role_id and secret_id
          echo "$VAULT_ROLE_ID" > role_id.tmp
          echo "$VAULT_SECRET_ID" > secret_id.tmp
          
          # Get token from Vault
          VAULT_TOKEN=$(vault write -tls-skip-verify -field=token auth/approle/login \
                          role_id=@role_id.tmp \
                          secret_id=@secret_id.tmp)
          
          # Remove temporary files
          rm role_id.tmp secret_id.tmp
          
          # Mask the token in logs for security
          echo "::add-mask::$VAULT_TOKEN"
          
          # Set output and env var
          echo "token=$VAULT_TOKEN" >> $GITHUB_OUTPUT
          echo "VAULT_TOKEN=$VAULT_TOKEN" >> $GITHUB_ENV

      - name: Get Runner IP and Update Firewall
        id: runner-ip
        env:
          DO_TOKEN: ${{ secrets.DO_TOKEN }}
        run: |
          # Get runner's public IP
          RUNNER_IP=$(curl -s https://ifconfig.me)
          echo "Runner IP: $RUNNER_IP"
          echo "::add-mask::$RUNNER_IP"
          echo "runner_ip=$RUNNER_IP" >> $GITHUB_OUTPUT
          
          # Set the IP for Terraform
          echo "TF_VAR_runner_ip=$RUNNER_IP" >> $GITHUB_ENV

      - name: Extract SSH Public Key
        id: ssh-public-key
        run: |
          # Create temporary private key file
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > /tmp/deploy_key
          chmod 600 /tmp/deploy_key
          
          # Extract public key from private key
          ssh-keygen -y -f /tmp/deploy_key > /tmp/deploy_key.pub
          
          # Read public key and set as environment variable
          SSH_PUBLIC_KEY=$(cat /tmp/deploy_key.pub)
          echo "::add-mask::$SSH_PUBLIC_KEY"
          echo "TF_VAR_ssh_public_key=$SSH_PUBLIC_KEY" >> $GITHUB_ENV
          
          # Clean up temporary files
          rm -f /tmp/deploy_key /tmp/deploy_key.pub
          
          echo "‚úÖ SSH public key extracted and set for Terraform"

      - name: Initialize Terraform
        working-directory: .cicd/terraform/compute
        env:
          # Credentials for DigitalOcean Spaces (S3-compatible backend)
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: terraform init

      - name: Debug Backend Configuration
        working-directory: .cicd/terraform/compute
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          echo "=== Backend Configuration Debug ==="
          
          # Check if local state exists (fallback mode)
          if [ -f terraform.tfstate ]; then
            echo "‚ùå WARNING: Local terraform.tfstate file exists!"
            ls -la terraform.tfstate
          else
            echo "‚úÖ No local terraform.tfstate found"
          fi
          
          # Check .terraform directory
          echo "=== .terraform directory contents ==="
          ls -la .terraform/ || echo "No .terraform directory"
          
          # Check backend configuration
          if [ -f .terraform/terraform.tfstate ]; then
            echo "=== Backend state file ==="
            cat .terraform/terraform.tfstate
          else
            echo "‚ùå No .terraform/terraform.tfstate found"
          fi
          
          # Test S3 connection manually
          echo "=== Testing S3 connection ==="
          
          # Try to list bucket contents
          aws s3 ls s3://grimwaves-terraform-state/ \
            --endpoint-url=https://fra1.digitaloceanspaces.com \
            || echo "‚ùå Failed to list bucket contents"
          
          # Test write access to bucket
          echo "test-$(date)" > test-connection.txt
          aws s3 cp test-connection.txt s3://grimwaves-terraform-state/test-connection.txt \
            --endpoint-url=https://fra1.digitaloceanspaces.com \
            && echo "‚úÖ Successfully wrote test file" \
            || echo "‚ùå Failed to write test file"
          
          # Clean up test file locally and remotely
          rm -f test-connection.txt
          aws s3 rm s3://grimwaves-terraform-state/test-connection.txt \
            --endpoint-url=https://fra1.digitaloceanspaces.com \
            2>/dev/null || echo "Test file already cleaned up"

      - name: Terraform Plan
        working-directory: .cicd/terraform/compute
        env:
          # Use environment variables from GitHub Environment if available
          TF_VAR_blue_green_enabled: ${{ vars.BLUE_GREEN_DEPLOYMENT || 'false' }}
          # Optional: Override domain if needed via GitHub Environment variables
          TF_VAR_domain_name: ${{ vars.DEPLOYMENT_DOMAIN || '' }}
          # Credentials for DigitalOcean Spaces (S3-compatible backend)
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          # Use dynamic IP for CI/CD runner instead of 0.0.0.0/0
          terraform plan \
            -var="do_token=${{ secrets.DO_TOKEN }}" \
            -var="ssh_key_fingerprint=${{ secrets.SSH_KEY_FINGERPRINT }}" \
            -var="environment=${{ needs.prepare.outputs.environment }}" \
            -var='allowed_ssh_cidr_blocks=["${{ steps.runner-ip.outputs.runner_ip }}/32", "${{ secrets.VPN_IP }}/32"]' \
            -var="emergency_ssh_access=true" \
            -out=tfplan

      - name: Terraform Apply
        working-directory: .cicd/terraform/compute
        env:
          # Credentials for DigitalOcean Spaces (S3-compatible backend)
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: terraform apply -auto-approve tfplan

      - name: Verify State File Upload
        working-directory: .cicd/terraform/compute
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          echo "=== Verifying state file was uploaded to bucket ==="
          
          # Check if local state file exists (should not exist with remote backend)
          if [ -f terraform.tfstate ]; then
            echo "‚ùå CRITICAL: Local terraform.tfstate exists! Remote backend failed!"
            echo "Local state file size: $(wc -c < terraform.tfstate)"
            echo "This means Terraform used local backend as fallback"
            exit 1
          else
            echo "‚úÖ No local terraform.tfstate found (good)"
          fi
          
          # Try to list bucket contents to verify state file exists
          echo "=== Checking bucket contents ==="
          aws s3 ls s3://grimwaves-terraform-state/ \
            --endpoint-url=https://fra1.digitaloceanspaces.com \
            --recursive || echo "‚ùå Failed to list bucket contents"
          
          # Check specifically for our state file
          if aws s3 ls s3://grimwaves-terraform-state/compute/terraform.tfstate \
            --endpoint-url=https://fra1.digitaloceanspaces.com; then
            echo "‚úÖ State file found in bucket!"
            
            # Get file size
            aws s3 ls s3://grimwaves-terraform-state/compute/terraform.tfstate \
              --endpoint-url=https://fra1.digitaloceanspaces.com \
              --human-readable --summarize
          else
            echo "‚ùå CRITICAL: State file NOT found in bucket!"
            echo "This indicates Terraform did not upload state to remote backend"
            exit 1
          fi
          
          # Try to read the state file to verify it contains our resources
          echo "=== Verifying state file contents ==="
          aws s3 cp s3://grimwaves-terraform-state/compute/terraform.tfstate - \
            --endpoint-url=https://fra1.digitaloceanspaces.com \
            | jq '.resources[] | select(.type == "digitalocean_project") | .instances[0].attributes.name' \
            || echo "‚ùå Failed to read/parse state file"

      - name: Get Outputs
        id: terraform-outputs
        working-directory: .cicd/terraform/compute
        env:
          # Credentials for DigitalOcean Spaces (S3-compatible backend)
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          echo "droplet_ip=$(terraform output -raw droplet_ipv4)" >> $GITHUB_OUTPUT
          echo "app_url=$(terraform output -raw app_url)" >> $GITHUB_OUTPUT
          echo "ssh_port=$(terraform output -raw ssh_port)" >> $GITHUB_OUTPUT

  deploy:
    if: ${{ false }}
    name: Deploy Application
    runs-on: ubuntu-latest
    needs: [prepare, terraform]
    # Use GitHub Environment - this enables environment-specific secrets and protection rules
    environment: ${{ needs.prepare.outputs.environment }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Wait for Droplet Initialization
        run: |
          echo "‚è≥ Waiting for droplet cloud-init to complete..."
          echo "Droplet IP: ${{ needs.terraform.outputs.droplet_ip }}"
          echo "SSH Port: ${{ needs.terraform.outputs.ssh_port }}"
          
          # Wait for cloud-init to finish (usually takes 2-5 minutes)
          echo "Waiting 3 minutes for cloud-init to complete SSH configuration..."
          sleep 180
          
          # Test if SSH port is open using netcat-like functionality with timeout
          echo "Testing SSH port availability..."
          for i in {1..12}; do
            if timeout 10 bash -c "</dev/tcp/${{ needs.terraform.outputs.droplet_ip }}/${{ needs.terraform.outputs.ssh_port }}"; then
              echo "‚úÖ SSH port ${{ needs.terraform.outputs.ssh_port }} is now available!"
              break
            else
              echo "‚è≥ Attempt $i: SSH port not ready yet, waiting 30 seconds..."
              sleep 30
            fi
          done

      - name: Get SSH Key from GitHub Secrets
        id: ssh-key
        run: |
          # Setup SSH key from GitHub secrets
          mkdir -p ~/.ssh
          
          # Get SSH private key from GitHub secrets instead of Vault
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          
          # Setup known_hosts dynamically using ssh-keyscan with custom port
          touch ~/.ssh/known_hosts
          echo "üîë Adding droplet to known_hosts (port ${{ needs.terraform.outputs.ssh_port }})..."
          
          # Retry ssh-keyscan a few times as droplet might still be starting
          for i in {1..5}; do
            if ssh-keyscan -p ${{ needs.terraform.outputs.ssh_port }} -H "${{ needs.terraform.outputs.droplet_ip }}" >> ~/.ssh/known_hosts 2>/dev/null; then
              echo "‚úÖ Successfully added droplet to known_hosts"
              break
            else
              echo "‚è≥ Attempt $i failed, waiting 10 seconds..."
              sleep 10
            fi
          done
          
          # Configure SSH to use custom port from Terraform
          echo "üîß Configuring SSH to use custom port ${{ needs.terraform.outputs.ssh_port }}..."
          cat >> ~/.ssh/config << EOF
          Host ${{ needs.terraform.outputs.droplet_ip }}
            Port ${{ needs.terraform.outputs.ssh_port }}
            StrictHostKeyChecking no
            UserKnownHostsFile /dev/null
            LogLevel ERROR
          EOF
          
          echo "üîß SSH configuration complete (port ${{ needs.terraform.outputs.ssh_port }})"

      - name: Copy Docker Compose files
        run: |
          # Prepare docker-compose files
          if [ "${{ needs.prepare.outputs.environment }}" == "production" ]; then
            TARGET_COMPOSE="docker-compose.yml docker-compose.prod.yml"
          else
            TARGET_COMPOSE="docker-compose.yml docker-compose.staging.yml"
          fi
          
          # Copy files to remote server using custom port from Terraform
          scp -P ${{ needs.terraform.outputs.ssh_port }} $TARGET_COMPOSE deploy@${{ needs.terraform.outputs.droplet_ip }}:/var/app/grimwaves/

      - name: Configure Vault Agent
        run: |
          # Create role_id and secret_id files
          echo "${{ secrets.VAULT_ROLE_ID }}" > role_id.tmp
          echo "${{ secrets.VAULT_SECRET_ID }}" > secret_id.tmp
          
          # Copy to server using custom port from Terraform
          scp -P ${{ needs.terraform.outputs.ssh_port }} role_id.tmp deploy@${{ needs.terraform.outputs.droplet_ip }}:/var/app/grimwaves/vault-agent/auth/role-id
          scp -P ${{ needs.terraform.outputs.ssh_port }} secret_id.tmp deploy@${{ needs.terraform.outputs.droplet_ip }}:/var/app/grimwaves/vault-agent/auth/secret-id
          
          # Clean up
          rm role_id.tmp secret_id.tmp

      - name: Run Deployment
        env:
          ENVIRONMENT: ${{ needs.prepare.outputs.environment }}
        run: |
          # Create .env file with the image tag and environment-specific variables
          {
            echo "DOCKER_TAG=${{ needs.prepare.outputs.tag }}"
            echo "ENVIRONMENT=$ENVIRONMENT"
            # Add any environment-specific variables from GitHub Environment
            if [ -n "${{ vars.ADDITIONAL_ENV_VARS }}" ]; then
              echo "${{ vars.ADDITIONAL_ENV_VARS }}"
            fi
          } > .env.tmp
          
          scp -P ${{ needs.terraform.outputs.ssh_port }} .env.tmp deploy@${{ needs.terraform.outputs.droplet_ip }}:/var/app/grimwaves/.env.docker
          
          # Run deployment script using custom port from Terraform
          ssh -p ${{ needs.terraform.outputs.ssh_port }} deploy@${{ needs.terraform.outputs.droplet_ip }} 'cd /var/app/grimwaves && sudo ./deploy.sh'
          
          # Clean up
          rm .env.tmp

      - name: Verify Deployment
        run: |
          # Wait for services to start
          echo "Waiting for services to start..."
          sleep 30
          
          # Check if the service is responding
          curl -sSf --retry 5 --retry-delay 10 "${{ needs.terraform.outputs.app_url }}/health" || (echo "Service health check failed!" && exit 1)
          
          echo "Deployment completed successfully!"

  notify:
    if: ${{ false }}
    name: Send Notifications
    runs-on: ubuntu-latest
    needs: [prepare, terraform, deploy]
    # if: always()
    # Note: This job does not use an environment context but still accesses environment-specific secrets
    steps:
      - name: Deployment Status
        id: status
        run: |
          if [[ "${{ needs.deploy.result }}" == "success" ]]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "message=‚úÖ Deployment to ${{ needs.prepare.outputs.environment }} succeeded!" >> $GITHUB_OUTPUT
          else
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "message=‚ùå Deployment to ${{ needs.prepare.outputs.environment }} failed!" >> $GITHUB_OUTPUT
          fi

      - name: Send Email Notification
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: ${{ secrets.MAIL_SERVER }}
          server_port: ${{ secrets.MAIL_PORT }}
          username: ${{ secrets.MAIL_USERNAME }}
          password: ${{ secrets.MAIL_PASSWORD }}
          subject: ${{ steps.status.outputs.message }}
          body: |
            Deployment Status: ${{ steps.status.outputs.status }}
            Environment: ${{ needs.prepare.outputs.environment }}
            Image Tag: ${{ needs.prepare.outputs.tag }}
            Application URL: ${{ needs.terraform.outputs.app_url }}
            
            See details at: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          to: ${{ secrets.MAIL_RECIPIENT }}
          from: GrimWaves CI/CD <${{ secrets.MAIL_USERNAME }}>

  e2e_tests:
    if: ${{ false }}
    name: E2E Tests
    runs-on: ubuntu-latest
    needs: deploy
    environment: ${{ needs.prepare.outputs.environment }}
    # Only run E2E tests for staging environment
    # if: needs.prepare.outputs.environment == 'staging'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: ./.github/actions/setup-python-poetry
        
      - name: Wait for deployment to stabilize
        run: |
          echo "Waiting for 30 seconds to ensure deployment is stable..."
          sleep 30
        
      - name: Run E2E Tests
        env:
          E2E_TEST_URL: https://api-staging.grimwaves.com
        run: |
          poetry run pytest tests/e2e/ -v
          
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-test-results
          path: |
            .pytest_cache/
            pytest-report.xml
          retention-days: 7 